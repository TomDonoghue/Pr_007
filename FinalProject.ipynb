{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project - Super Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import floor\n",
    "import patsy\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many cities and neighbourhoods in the United States, such as Los Angeles have been designed for extensive car use. While these models might have worked in the past, this dynamic is beginning to change as the result of overpopulation, rideshares and automation among other factors. Urban neighborhoods are no longer just places of living for large amounts of people, but rather an intertwined mesh of commercial activity, transport and residences (3). This does not fit the originally intended model - it is not efficient, causes pollution, and prevents children from being able to play in the streets.\n",
    "\n",
    "### In Barcelona(2), city planners realized this problem and implemented a system where they blocked off certain urban areas to vehicle traffic, opening them up for commercial and community use. The results were astounding - with an increase shown in community engagement along with a decrease in pollution. Economic benefits have also been shown for the local businesses in these \"superblocks\" (4,5). As a result, we have decided to create a model that can figure out the best blocks to block out for such activity, geospatially and temporally.\n",
    "\n",
    "### We will be using the number of restaurants, coffee shops, bars and transit stations to categorize the 'walkability' of different blocks, building a model analagous to walkscore (1). Furthermore, we will be using the Yelp API to temporally analyze foot traffic based on numbers and timing of reviews and checkins.\n",
    "\n",
    "##### Referencess:\n",
    "  ######  1)https://www.walkscore.com/professional/research.php\n",
    "   ###### 2)https://www.theguardian.com/cities/2016/may/17/superblocks-rescue-barcelona-spain-plan-give-streets-back-residents\n",
    "   ###### 3)https://www.wired.com/2017/04/brilliant-simplicity-new-yorks-new-times-square/ \n",
    "   ###### 4)http://krqe.com/2017/03/06/city-councilor-wants-wider-sidewalks-to-help-businesses-impacted-by-art/\n",
    "   ###### 5)http://www.nyc.gov/html/dot/downloads/pdf/dot-economic-benefits-of-sustainable-streets.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "'FY2014 Ridership_Trolley_Sept2013Booking.csv' has the information about the number of people getting on and off the trolley for each ride for Sept2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "'FY2014 Ridership_Trolley_Sept2013_Stops.csv' has the information about the location and other information about the trolley locations in San Diego."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning / Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active Businesses (Restaurants, cafe's etc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Importing the original file for active business data in SD\n",
    "\n",
    "fname = \"Restaurant_Counts.csv\"\n",
    "business_df = pd.read_csv(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Importing the original file for active business data in SD\n",
    "\n",
    "fname1 = \"Stop_Counts.csv\"\n",
    "stops_df = pd.read_csv(fname1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Importing the original file for active business data in SD\n",
    "\n",
    "fname2 = \"Parking_Counts.csv\"\n",
    "parking_df = pd.read_csv(fname2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Removing unneccesary columns and renaming to layman terms, filtering all business into places that serve food/drinks\n",
    "\n",
    "business_df = business_df[['doing_bus_as_name','zip','naics_description','lat','lon']]\n",
    "business_df = business_df.loc[(business_df['naics_description'] == 'full-service restaurants') |\n",
    "               (business_df['naics_description'] == 'cafeterias') | \n",
    "               (business_df['naics_description'] == 'food services & drinking places') |\n",
    "               (business_df['naics_description'] == 'limited-service eating places') |\n",
    "               (business_df['naics_description'] == 'limited-service restaurants')  |\n",
    "               (business_df['naics_description'] == 'mobile food services') |\n",
    "               (business_df['naics_description'] == 'drinking places (alcoholic beverages)') |\n",
    "               (business_df['naics_description'] == 'snack & nonalcoholic beverage bars')]\n",
    "business_df.rename(columns = {'doing_bus_as_name':'Business title','naics_description':'Type of Place'}, inplace=True)\n",
    "business_df = business_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Adding the 5 columns for the different time brackets\n",
    "\n",
    "business_df.insert(2,'AM early','Null')\n",
    "business_df.insert(3,'AM peak','Null')\n",
    "business_df.insert(4,'Mid-day','Null')\n",
    "business_df.insert(5,'PM peak','Null')\n",
    "business_df.insert(6,'PM late','Null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generalising the different categories into 3 categories: Only Food, Food & Drinks, Only Drinks\n",
    "\n",
    "business_df.loc[(business_df['Type of Place'] == 'mobile food services') |\n",
    "                (business_df['Type of Place'] == 'cafeterias') |\n",
    "                (business_df['Type of Place'] == 'snack & nonalcoholic beverage bars') |\n",
    "                (business_df['Type of Place'] == 'limited-service eating places') |\n",
    "                (business_df['Type of Place'] == 'limited-service restaurants') \n",
    "                , 'Type of Place'] = 'Only Food'\n",
    "\n",
    "business_df.loc[(business_df['Type of Place'] == 'full-service restaurants') \n",
    "                , 'Type of Place'] = 'Food & Drinks'\n",
    "\n",
    "business_df.loc[(business_df['Type of Place'] == 'drinking places (alcoholic beverages)') |\n",
    "                (business_df['Type of Place'] == 'food services & drinking places') \n",
    "                , 'Type of Place'] = 'Only Drinks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Assigning values to different time brackets by assuming foot traffic according to the type of place \n",
    "\n",
    "business_df.loc[(business_df['Type of Place'] == 'Only Food'), \n",
    "                ('AM early','AM peak','Mid-day','PM peak','PM late')] = ('3','25','20','18','5')\n",
    "business_df.loc[(business_df['Type of Place'] == 'Food & Drinks'),\n",
    "                ('AM early','AM peak','Mid-day','PM peak','PM late')] = ('7','28','25','35','25')\n",
    "business_df.loc[(business_df['Type of Place'] == 'Only Drinks'),\n",
    "                ('AM early','AM peak','Mid-day','PM peak','PM late')] = ('27','6','3','27','30') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Assigning a total score to all three categories (summ of scores of all time brackets)\n",
    "\n",
    "business_df.loc[(business_df['Type of Place'] == 'Only Food'), \n",
    "                'Total Score'] = '71'\n",
    "business_df.loc[(business_df['Type of Place'] == 'Food & Drinks'),\n",
    "                'Total Score'] = '120'\n",
    "business_df.loc[(business_df['Type of Place'] == 'Only Drinks'),\n",
    "                'Total Score'] = '93'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "business_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "business_df.to_csv('/Users/Devyaanshu/Pr_007/restaurants_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transit Stops Dataset\n",
    "The following cells take the original dataset and clean it so that we get the cleaned data, NOT MEANT TO BE RUN since we already have the cleaned dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#Code which we used to clean the original dataset which is way too big for Github \n",
    "\n",
    "trips = pd.read_csv('FY2014 Ridership_Trolley_Sept2013Booking.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Removing unnecessary columns: all we need is stop_id, passengers getting off on the station \n",
    "# and time_arrival to get the time split for the number of people getting off at a station at a time_period\n",
    "\n",
    "trips = trips[['STOP_ID', 'PASSENGERS_OFF', 'TIME_ACTUAL_ARRIVE']]\n",
    "trips.columns = ['stop_id', 'count', 'time']\n",
    "trips"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### clean the data, removing any na rows we will also remove all rows that have value 0\n",
    "\n",
    "trips.dropna(how='any')\n",
    "trips = trips[trips['count'] != 0]\n",
    "trips"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Using stop_id we can connect with lat/long. We will be grouping by pole_id to create a table with \n",
    "# the following columns: (stop_id, latitude, longitude, count_am_early, count_am_peak, count_midday, \n",
    "# count_pm_early, count_pm_late, count_daily)\n",
    "\n",
    "#### Getting location data for all the stops using another dataset\n",
    "stop_locs = pd.read_csv('FY2014 Ridership_Trolley_Sept2013_Stops.csv')\n",
    "stop_locs = stop_locs[['STOP_ID', 'LAT', 'LON']]\n",
    "stop_locs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "stop_locs.columns = ['stop_id', 'latitude', 'longitude']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "stops_df = trips.merge(stop_locs, how='left')\n",
    "stops_df = stops_df.dropna(how='any')\n",
    "stops_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### grouping by stop_id and aggregating over the count values\n",
    "\n",
    "stop_counts = stops_df.groupby(['stop_id']).agg('count')\n",
    "\n",
    "##### remove unnecessary columns\n",
    "\n",
    "stop_counts = stop_counts['time'] \n",
    "stop_counts = stop_counts.to_frame()\n",
    "stop_counts['stop_id'] = stop_counts.index\n",
    "stop_counts = stop_counts.merge(stop_locs, how='left')\n",
    "stop_counts.columns = ['total_count', 'stop_id', 'latitude', 'longitude']\n",
    "stop_counts"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### We need to now find the number of days in the transactions dataset\n",
    "##### We will be using this in order to get the count of transactions PER DAY\n",
    "\n",
    "stops_df['time'] = pd.to_datetime(stops_df['time'])\n",
    "dates = stops_df['time']\n",
    "am_early_d = {}\n",
    "am_peak_d = {}\n",
    "midday_d = {}\n",
    "pm_peak_d = {}\n",
    "pm_late_d = {}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "##### The time ranges for which we have split the transit ridership data are:\n",
    "AM_Early = 12AM-6AM\n",
    "AM_Late = 6AM-9AM\n",
    "..\n",
    "..\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Classify the time slot based on the times during the day\n",
    "def classify(x): \n",
    "    hour = x.time().hour\n",
    "    if hour <=6:\n",
    "        return 'am_early'\n",
    "    elif hour <=9:\n",
    "        return 'am_peak'\n",
    "    elif hour <=14:\n",
    "        return 'midday'\n",
    "    elif hour <=19:\n",
    "        return 'pm_peak'\n",
    "    else:\n",
    "        return 'pm_late'\n",
    "stops_df['time_slot'] = stops_df['time'].apply(classify)\n",
    "stops_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Merging the two datasets by key value and column name\n",
    "\n",
    "def checkSeriesColumn(s, col):\n",
    "    val = False\n",
    "    for row in s.keys().to_series().str.contains(col): \n",
    "        if(row == True):\n",
    "            val = True\n",
    "    return val"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Setting the values of the transactions for each time period during the day\n",
    "def set_temporal_counts(p_id):\n",
    "    v_counts = stops_df.loc[stops_df['stop_id'] == p_id]['time_slot'].value_counts(dropna=False)\n",
    "    stop_counts.loc[stop_counts['stop_id'] == p_id,'am_early'] = 0 if not checkSeriesColumn(v_counts, 'am_early') else v_counts['am_early']\n",
    "    stop_counts.loc[stop_counts['stop_id'] == p_id,'am_peak'] =  0 if not checkSeriesColumn(v_counts, 'am_peak') else v_counts['am_peak']\n",
    "    stop_counts.loc[stop_counts['stop_id'] == p_id,'midday'] =  0 if not checkSeriesColumn(v_counts, 'midday') else v_counts['midday']\n",
    "    stop_counts.loc[stop_counts['stop_id'] == p_id,'pm_peak'] =  0 if not checkSeriesColumn(v_counts, 'pm_peak') else v_counts['pm_peak']\n",
    "    stop_counts.loc[stop_counts['stop_id'] == p_id,'pm_late'] = 0 if not checkSeriesColumn(v_counts, 'pm_late') else v_counts['pm_late']\n",
    "\n",
    "stop_counts['stop_id'].apply(set_temporal_counts)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "stop_counts.to_csv('Stop_Counts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cleaned dataset for the transit spots\n",
    "stops_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parking meters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Importing data and initial cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#importing dataset\n",
    "parking = pd.read_csv('treas_parking_payments_2017_datasd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#removing unnecessary columns: all we need is pole_id, time_start and meter_expire\n",
    "parking = parking.drop(['uuid'], axis=1)\n",
    "parking = parking.drop(['trans_amt'], axis=1)\n",
    "parking = parking.drop(['pay_method'], axis=1)\n",
    "parking = parking.drop(['meter_type'], axis=1)\n",
    "parking = parking.drop(['meter_expire'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clean the data, removing any na rows \n",
    "parking.dropna(how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Connecting with other dataset (with lat/long pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2.) using pole_id we can connect with lat/long. We will be grouping by pole_id to create\n",
    "#     a table with the following columns:\n",
    "#     (pole_id, latitude, longitude, count_am_early, count_am_peak, \n",
    "#        count_midday, count_pm_early, count_pm_late, count_daily)\n",
    "# NOTE: we will need to decide whether to use raw numbers or averages of counts per section per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "park_loc = pd.read_csv('treas_parking_meters_loc_datasd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "park_loc = park_loc[['pole', 'longitude', 'latitude']]\n",
    "park_loc.columns = ['pole_id', 'longitude', 'latitude']\n",
    "park_loc = park_loc.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "park_df = parking.merge(park_loc, how='left')\n",
    "park_df = park_df.dropna(how='any')\n",
    "park_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "park_counts = park_df.groupby(['pole_id']).agg('count')\n",
    "park_counts = park_counts['trans_start'] #remove unnecessary columns\n",
    "park_counts = park_counts.to_frame()\n",
    "park_counts['pole_id'] = park_counts.index\n",
    "park_counts = park_counts.merge(park_loc, how='left')\n",
    "park_counts.columns = ['total_count', 'pole_id', 'longitude', 'latitude']\n",
    "park_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we need to now find the number of days in the transactions dataset\n",
    "# we will be using this in order to get the count of transactions PER DAY\n",
    "park_df['trans_start'] = pd.to_datetime(park_df['trans_start'])\n",
    "dates = park_df['trans_start']\n",
    "am_early_d = {}\n",
    "am_peak_d = {}\n",
    "midday_d = {}\n",
    "pm_peak_d = {}\n",
    "pm_late_d = {}\n",
    "\n",
    "def classify(x): \n",
    "    hour = x.time().hour\n",
    "    if hour <=6:\n",
    "        return 'am_early'\n",
    "    elif hour <=9:\n",
    "        return 'am_peak'\n",
    "    elif hour <=14:\n",
    "        return 'midday'\n",
    "    elif hour <=19:\n",
    "        return 'pm_peak'\n",
    "    else:\n",
    "        return 'pm_late'\n",
    "park_df['time_slot'] = park_df['trans_start'].apply(classify)\n",
    "park_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkSeriesColumn(s, col):\n",
    "    val = False\n",
    "    for row in s.keys().to_series().str.contains(col): \n",
    "        if(row == True):\n",
    "            val = True\n",
    "    return val\n",
    "def set_temporal_counts(p_id):\n",
    "    v_counts = park_df.loc[park_df['pole_id'] == p_id]['time_slot'].value_counts(dropna=False)\n",
    "    park_counts.loc[park_counts['pole_id'] == p_id,'am_early'] = 0 if not checkSeriesColumn(v_counts, 'am_early') else v_counts['am_early']\n",
    "    park_counts.loc[park_counts['pole_id'] == p_id,'am_peak'] =  0 if not checkSeriesColumn(v_counts, 'am_peak') else v_counts['am_peak']\n",
    "    park_counts.loc[park_counts['pole_id'] == p_id,'midday'] =  0 if not checkSeriesColumn(v_counts, 'midday') else v_counts['midday']\n",
    "    park_counts.loc[park_counts['pole_id'] == p_id,'pm_peak'] =  0 if not checkSeriesColumn(v_counts, 'pm_peak') else v_counts['pm_peak']\n",
    "    park_counts.loc[park_counts['pole_id'] == p_id,'pm_late'] = 0 if not checkSeriesColumn(v_counts, 'pm_late') else v_counts['pm_late']\n",
    "#     park_counts.loc[park_counts['pole_id'] == ]\n",
    "#      park_counts.loc[park_counts['pole_id'] == p_id,'am_early'] = 0 if not checkSeriesColumn(v_counts, 'am_early') else v_counts['am_early']\n",
    "#      park_counts.loc[park_counts['pole_id'] == p_id,'am_peak'] =  0 if not checkSeriesColumn(v_counts, 'am_peak') else v_counts['am_peak']\n",
    "#     park_counts.loc[park_counts['pole_id'] == p_id,'midday'] =  0 if not checkSeriesColumn(v_counts, 'midday') else v_counts['midday']\n",
    "#     park_counts.loc[park_counts['pole_id'] == p_id,'pm_peak'] =  0 if not checkSeriesColumn(v_counts, 'pm_peak') else v_counts['pm_peak']\n",
    "#     park_counts.loc[park_counts['pole_id'] == p_id,'pm_late'] = 0 if not checkSeriesColumn(v_counts, 'pm_late') else v_counts['pm_late']\n",
    "\n",
    "\n",
    "park_counts['pole_id'].apply(set_temporal_counts)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# v_counts = park_df.loc[park_df['pole_id'] == 'N-1003']['time_slot'].value_counts(dropna=False)\n",
    "# park_counts.loc[park_counts['pole_id'] == 'N-1003','am_early'] = 0 if not checkSeriesColumn(v_counts, 'am_early') else v_counts['am_early']\n",
    "# park_counts.loc[park_counts['pole_id'] == 'N-1003','am_peak'] =  0 if not checkSeriesColumn(v_counts, 'am_peak') else v_counts['am_peak']\n",
    "# park_counts.loc[park_counts['pole_id'] == 'N-1003','midday'] =  0 if not checkSeriesColumn(v_counts, 'midday') else v_counts['midday']\n",
    "# park_counts.loc[park_counts['pole_id'] == 'N-1003','pm_peak'] =  0 if not checkSeriesColumn(v_counts, 'pm_peak') else v_counts['pm_peak']\n",
    "# park_counts.loc[park_counts['pole_id'] == 'N-1003','pm_late'] = 0 if not checkSeriesColumn(v_counts, 'pm_late') else v_counts['pm_late']\n",
    "# park_counts.loc[park_counts['pole_id'] == 'N-1003']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_date = dates.max()\n",
    "min_date = dates.min()\n",
    "days_elapsed = (max_date - min_date).days + 1 #to round off this number\n",
    "days_elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "park_counts.to_csv(\"Parking_Counts.csv\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's now get average counts of parked vehicles\n",
    "# dividing by total days to give per daily expected counts \n",
    "park_counts['total_count'] = park_counts['total_count'].apply(lambda x: x / days_elapsed)\n",
    "park_counts['am_early'] = park_counts['am_early'].apply(lambda x: x / days_elapsed)\n",
    "park_counts['am_peak'] = park_counts['am_peak'].apply(lambda x: x / days_elapsed)\n",
    "park_counts['midday'] = park_counts['midday'].apply(lambda x: x / days_elapsed)\n",
    "park_counts['pm_peak'] = park_counts['pm_peak'].apply(lambda x: x / days_elapsed)\n",
    "park_counts['pm_late'] = park_counts['pm_late'].apply(lambda x: x / days_elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we then multiply this amount by the mean number of people per vehicle\n",
    "# as per https://www.rita.dot.gov/bts/sites/rita.dot.gov.bts/files/publications/highlights_of_the_2001_national_household_travel_survey/html/table_a15.html\n",
    "ppl_per_vehicle = 1.63\n",
    "park_counts['total_count'] = park_counts['total_count'].apply(lambda x: x * ppl_per_vehicle)\n",
    "park_counts['am_early'] = park_counts['am_early'].apply(lambda x: x * ppl_per_vehicle)\n",
    "park_counts['am_peak'] = park_counts['am_peak'].apply(lambda x: x * ppl_per_vehicle)\n",
    "park_counts['midday'] = park_counts['midday'].apply(lambda x:  x * ppl_per_vehicle)\n",
    "park_counts['pm_peak'] = park_counts['pm_peak'].apply(lambda x:  x * ppl_per_vehicle)\n",
    "park_counts['pm_late'] = park_counts['pm_late'].apply(lambda x:  x * ppl_per_vehicle)\n",
    "park_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "park_counts = pd.read_csv(\"Parking_Counts.csv\");\n",
    "####Rough Calculations for algorithm\n",
    "long_max = park_counts['longitude'].max()\n",
    "long_min = park_counts['longitude'].min()\n",
    "lat_max = park_counts['latitude'].max()\n",
    "lat_min = park_counts['latitude'].min()\n",
    "lat_dif = lat_max - lat_min\n",
    "long_dif = long_max - long_min\n",
    "NUMBER_BLOCKS_ROOT = 10 #this means 100 blocks 10x10\n",
    "lat_gap = lat_dif / NUMBER_BLOCKS_ROOT\n",
    "long_gap = long_dif / NUMBER_BLOCKS_ROOT\n",
    "\n",
    "def classify_blocks(s):\n",
    "    park_counts.loc[park_counts['pole_id'] == s,'row'] =  (park_counts.loc[park_counts['pole_id'] == s,'latitude'] - lat_min) // lat_gap\n",
    "    park_counts.loc[park_counts['pole_id'] == s,'col'] = (park_counts.loc[park_counts['pole_id'] == s,'longitude'] - long_min) // long_gap\n",
    "\n",
    "park_counts['pole_id'].apply(classify_blocks)\n",
    "\n",
    "park_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(long_max)\n",
    "print(long_min)\n",
    "print(long_dif)\n",
    "print(lat_max)\n",
    "print(lat_min)\n",
    "print(lat_dif)\n",
    "park_counts = park_counts.loc[park_counts['longitude'] != -180.0 ] #remove the outlier\n",
    "long_min = park_counts['longitude'].min()\n",
    "long_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "park_counts.to_csv(\"Parking_Counts_grid.csv\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis and Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "park_counts = pd.read_csv('Parking_Counts.csv')\n",
    "park_counts = park_counts.loc[park_counts['longitude'] != -180.0 ] #remove the outlier\n",
    "park_counts.rename(columns={'pole_id':'id'}, inplace= True)\n",
    "park_counts['type'] = 'parking'\n",
    "del park_counts['Unnamed: 0']\n",
    "park_counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "restaurants = pd.read_csv('Restaurant_Counts.csv')\n",
    "restaurants.columns = ['id', 'title', 'zip', 'am_early', 'am_peak', 'midday', 'pm_peak', 'pm_late', 'type', 'latitude', 'longitude', 'total_count']\n",
    "restaurants['total_count'] = restaurants['total_count'].apply( lambda x : x * 2/3)\n",
    "restaurants['am_peak'] = restaurants['am_peak'].apply( lambda x : x * 2/3)\n",
    "restaurants['am_early'] = restaurants['am_early'].apply( lambda x : x * 2/3)\n",
    "restaurants['midday'] = restaurants['midday'].apply( lambda x : x * 2/3)\n",
    "restaurants['pm_peak'] = restaurants['pm_peak'].apply( lambda x : x * 2/3)\n",
    "restaurants['pm_late'] = restaurants['pm_late'].apply( lambda x : x * 2/3)\n",
    "restaurants['id'] = restaurants['id'].apply(lambda x : ('R-' + str(x) ))\n",
    "restaurants = restaurants[['id','type','title', 'am_early', 'am_peak', 'midday', 'pm_peak', 'pm_late', 'total_count','latitude', 'longitude']]\n",
    "\n",
    "restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transit = pd.read_csv('Stop_Counts.csv')\n",
    "transit.rename(columns={'stop_id':'id'}, inplace= True)\n",
    "transit['type'] = 'transit_stop'\n",
    "del transit['Unnamed: 0'] \n",
    "transit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "location_counts = restaurants.append(park_counts).append(transit)\n",
    "# Setting a new index\n",
    "#location_counts.index = idx # new ad hoc index\n",
    "# location_counts.index = range(len(location_counts)) # set with list\n",
    "# location_counts = location_counts.reset_index() # replace old w new\n",
    "# location_counts.rename(columns={'index':'idx'}, inplace= True)\n",
    "# location_counts.rename(columns={'id':'type_id'}, inplace= True)\n",
    "# del location_counts['Unnamed: 0']\n",
    "location_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "long_max = location_counts['longitude'].max()\n",
    "long_min = location_counts['longitude'].min()\n",
    "lat_max = location_counts['latitude'].max()\n",
    "lat_min = location_counts['latitude'].min()\n",
    "lat_dif = lat_max - lat_min\n",
    "long_dif = long_max - long_min\n",
    "NUMBER_BLOCKS_ROOT = 10 #this means 100 blocks 10x10\n",
    "lat_gap = lat_dif / NUMBER_BLOCKS_ROOT\n",
    "long_gap = long_dif / NUMBER_BLOCKS_ROOT\n",
    "\n",
    "def classify_blocks(s):\n",
    "    location_counts.loc[location_counts['id'] == s,'row'] =  (location_counts.loc[location_counts['id'] == s,'latitude'] - lat_min) // lat_gap\n",
    "    location_counts.loc[location_counts['id'] == s,'col'] = (location_counts.loc[location_counts['id'] == s,'longitude'] - long_min) // long_gap\n",
    "\n",
    "location_counts['id'].apply(classify_blocks)\n",
    "\n",
    "location_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(location_counts['row'].min())\n",
    "print(location_counts['row'].max())\n",
    "print(location_counts['row'].mean())\n",
    "print(location_counts['row'].std())\n",
    "print(location_counts['col'].min())\n",
    "print(location_counts['col'].max())\n",
    "print(location_counts['col'].mean())\n",
    "print(location_counts['col'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid = []\n",
    "for i in range(NUMBER_BLOCKS_ROOT):\n",
    "    for j in range(NUMBER_BLOCKS_ROOT):\n",
    "        grid.append([i, j, (lat_min+ i*lat_gap), (long_min + j*long_gap)])    \n",
    "grid = pd.DataFrame(grid)\n",
    "grid.columns = ['row','col','lat','lon']\n",
    "grid\n",
    "#grid.loc[(grid['row']==142) & (grid['col']==378)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "location_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "location_counts.to_csv('Location_grid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_loc_counts = location_counts[['total_count','row','col','latitude','longitude']]\n",
    "total_loc_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weighted_grid = total_loc_counts.groupby(['row','col']).sum().sort_values(by='total_count',ascending=False)\n",
    "weighted_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO:: Figure out how to merge these two dataframes where the values in grid \n",
    "#weighted_grid = pd.merge(weighted_grid, grid, on=['row')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions/Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
